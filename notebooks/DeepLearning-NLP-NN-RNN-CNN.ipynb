{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, We will use three different types of deep neural networks: Densely connected neural network (Basic Neural Network), Convolutional Neural Network (CNN) and Long Short Term Memory Network (LSTM), which is a variant of Recurrent Neural Networks. Furthermore, we will see how to evaluate deep learning model on a totally unseen data.\n",
    "\n",
    "Note: **This tutorial uses Keras Embedding Layer and GloVe word embeddings to convert text to numeric form**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "The dataset that can be downloaded from this Kaggle link.\n",
    "\n",
    "If you download the dataset and extract the compressed file, you will see a CSV file. The file contains 50,000 records and two columns: review and sentiment. The review column contains text for the review and the sentiment column contains sentiment for the review. The sentiment column can have two values i.e. \"positive\" and \"negative\" which makes our problem a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Libraries\n",
    "The following script imports the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in d:\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in d:\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in d:\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in d:\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in d:\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in d:\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in d:\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in d:\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.25.2)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in d:\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.60.1)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in d:\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in d:\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in d:\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in d:\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in d:\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in d:\\anaconda3\\lib\\site-packages (2.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\onayr\\AppData\\Local\\Temp\\ipykernel_56944\\3494678436.py\", line 8, in <module>\n",
      "    from keras.preprocessing.text import one_hot\n",
      "  File \"d:\\anaconda3\\Lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\n",
      "    from keras import __internal__\n",
      "  File \"d:\\anaconda3\\Lib\\site-packages\\keras\\__internal__\\__init__.py\", line 3, in <module>\n",
      "    from keras.__internal__ import backend\n",
      "  File \"d:\\anaconda3\\Lib\\site-packages\\keras\\__internal__\\backend\\__init__.py\", line 3, in <module>\n",
      "    from keras.src.backend import _initialize_variables as initialize_variables\n",
      "  File \"d:\\anaconda3\\Lib\\site-packages\\keras\\src\\__init__.py\", line 21, in <module>\n",
      "    from keras.src import applications\n",
      "  File \"d:\\anaconda3\\Lib\\site-packages\\keras\\src\\applications\\__init__.py\", line 41, in <module>\n",
      "    from keras.src.applications.inception_resnet_v2 import InceptionResNetV2\n",
      "  File \"d:\\anaconda3\\Lib\\site-packages\\keras\\src\\applications\\inception_resnet_v2.py\", line 324, in <module>\n",
      "    @keras.utils.register_keras_serializable()\n",
      "     ^^^^^^^^^^^\n",
      "AttributeError: partially initialized module 'keras.src' has no attribute 'utils' (most likely due to a circular import)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\Lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\Lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\Lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"d:\\anaconda3\\Lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Analyzing the Dataset\n",
    "Let's now import and analyze our dataset. Execute the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews = pd.read_csv(\"../data/IMDB Dataset.csv\")\n",
    "movie_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.isnull().values.any().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the script above we use the read_csv() method of the pandas library to read the CSV file containing our dataset. In the next line, we check if the dataset contains any NULL value or not. Finally, we print the shape of our dataset.\n",
    "\n",
    "Let's now print the first 5 rows of the dataset using the head() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take a look at any one of the reviews so that we have an idea about the text that we are going to process. Look at the following script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews[\"review\"][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that our text contains punctuations, brackets, and a few HTML tags as well. We will preprocess this text in the next section.\n",
    "\n",
    "Finally, let's see the distribution of positive and negative sentiments in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='sentiment', ylabel='count'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(x='sentiment', data=movie_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, it is clear that the dataset contains equal number of positive and negative reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "We saw that our dataset contained punctuations and HTML tags. In this section we will define a function that takes a text string as a parameter and then performs preprocessing on the string to remove special characters and HTML tags from the string. Finally, the string is returned to the calling function. Look at the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preprocess_text() method the first step is to remove the HTML tags. To remove the HTML tags, remove_tags() function has been defined. The remove_tags function simply replaces anything between opening and closing <> with an empty space.\n",
    "\n",
    "Next, in the preprocess_text function, everything is removed except capital and small English letters, which results in single characters that make no sense. For instance, when you remove apostrophe from the word \"Mark's\", the apostrophe is replaced by an empty space. Hence, we are left with single character \"s\".\n",
    "\n",
    "Next, we remove all the single characters and replace it by a space which creates multiple spaces in our text. Finally, we remove the multiple spaces from our text as well.\n",
    "\n",
    "Next, we will preprocess our reviews and will store them in a new list as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = []\n",
    "sentences = list(movie_reviews['review'])\n",
    "for sen in sentences:\n",
    "    processed_text.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now again see the fourth review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Basically there a family where little boy Jake thinks there a zombie in his closet his parents are fighting all the time This movie is slower than soap opera and suddenly Jake decides to become Rambo and kill the zombie OK first of all when you re going to make film you must Decide if its thriller or drama As drama the movie is watchable Parents are divorcing arguing like in real life And then we have Jake with his closet which totally ruins all the film expected to see BOOGEYMAN similar movie and instead watched drama with some meaningless thriller spots out of just for the well playing parents descent dialogs As for the shots with Jake just ignore them '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, you can see that the HTML tags, punctuations and numbers have been removed. We are only left with the alphabets.\n",
    "\n",
    "Next, we need to convert our labels into digits. Since we only have two labels in the output i.e. \"positive\" and \"negative\". We can simply convert them into integers by replacing \"positive\" with digit 1 and negative with digit 0 as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = movie_reviews['sentiment']\n",
    "\n",
    "y = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed_text_target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m processed_text_target\n",
      "\u001b[1;31mNameError\u001b[0m: name 'processed_text_target' is not defined"
     ]
    }
   ],
   "source": [
    "processed_text_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to divide our dataset into train and test sets. The train set will be used to train our deep learning models while the test set will be used to evaluate how well our model performs.\n",
    "\n",
    "We can use train_test_split method from the sklearn.model.selection module, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(processed_text, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script above divides our data into 80% for the training set and 20% for the testing set.\n",
    "\n",
    "Let's now write the script for our embedding layer. The embedding layer converts our textual data into numeric data and is used as the first layer for the deep learning models in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Embedding Layer\n",
    "As a first step, we will use the Tokenizer class from the keras.preprocessing.text module to create a word-to-index dictionary. In the word-to-index dictionary, each word in the corpus is used as a key, while a corresponding unique index is used as the value for the key. Execute the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you view the X_train variable in variable explorer, you will see that it contains 40,000 lists where each list contains integers. Each list actually corresponds to each sentence in the training set. You will also notice that the size of each list is different. This is because sentences have different lengths.\n",
    "\n",
    "We set the maximum size of each list to 100. You can try a different size. The lists with size greater than 100 will be truncated to 100. For the lists that have length less than 100, we will add 0 at the end of the list until it reaches the max length. This process is called padding.\n",
    "\n",
    "The following script finds the vocabulary size and then perform padding on both train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you view the X_train or X_test, you will see that all the lists have same length i.e. 100. Also, the vocabulary_size variable now contains a value 92547 which means that our corpus has 92547 unique words.\n",
    "\n",
    "We will use GloVe embeddings to create our feature matrix. In the following script we load the GloVe word embeddings and create a dictionary that will contain words as keys and their corresponding embedding list as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will create an embedding matrix where each row number will correspond to the index of the word in the corpus. The matrix will have 100 columns where each column will contain the GloVe word embeddings for the words in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you execute the above script, you will see that embedding_matrix will contain 92547 rows (one for each word in the corpus). Now we are ready to create our deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification with Simple Neural Network\n",
    "The first deep learning model that we are going to develop is a simple deep neural network. Look at the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the script above, we create a Sequential() model. Next, we create our embedding layer. The embedding layer will have an input length of 100, the output vector dimension will also be 100. The vocabulary size will be 92547 words. Since we are not training our own embeddings and using the GloVe embedding, we set trainable to False and in the weights attribute we pass our own embedding matrix.\n",
    "\n",
    "The embedding layer is then added to our model. Next, since we are directly connecting our embedding layer to densely connected layer, we flatten the embedding layer. Finally, we add a dense layer with sigmoid activation function.\n",
    "\n",
    "To compile our model, we will use the adam optimizer, binary_crossentropy as our loss function and accuracy as metrics and then we will print the summary of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 92547 words in our corpus and each word is represented as a 100-dimensional vector, the number of trainable parameter will be 92547x100 in the embedding layer. In the flattening layer, we simply multiply rows and column. Finally in the dense layer the number of parameters are 10000 (from the flattening layer) and 1 for the bias parameter, for a total of 10001.\n",
    "\n",
    "Let's now train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the scipt above, we use the fit method to train our neural network. Notice we are training on our train set only. The validation_split of 0.2 means that 20% of the training data is used to find the training accuracy of the algorithm.\n",
    "\n",
    "At the end of the training, you will see that training accuracy is around 85.52%.\n",
    "\n",
    "To evaluate the performance of the model, we can simply pass the test set to the evaluate method of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the test accuracy and loss, execute the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you execute the above script, you will see that we get a test accuracy of 74.68%. Our training accuracy was 85.52%. This means that our model is overfitting on the training set. Overfitting occurs when your model performs better on the training set than the test set. Ideally, the performance difference between training and test sets should be minimum.\n",
    "\n",
    "Let's try to plot the loss and accuracy differences for training and test sets. Execute the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see the differences for loss and accuracy between the training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification with a Convolutional Neural Network\n",
    "Convolutional neural network is a type of network that is primarily used for 2D data classification, such as images. A convolutional network tries to find specific features in an image in the first layer. In the next layers, the initially detected features are joined together to form bigger features. In this way, the whole image is detected.\n",
    "\n",
    "Convolutional neural networks have been found to work well with text data as well. Though text data is one-dimensional, we can use 1D convolutional neural networks to extract features from our data. To learn more about convolutional neural networks, please refer to this article.\n",
    "\n",
    "Let's create a simple convolutional neural network with 1 convolutional layer and 1 pooling layer. Remember, the code up to the creation of the embedding layer will remain same, execute the following piece of code after you create the embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np    \n",
    "from keras.layers.convolutional import Conv1D    \n",
    "from keras.models import Sequential    \n",
    "from keras.optimizers import Adam   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above script we create a sequential model, followed by an embedding layer. This step is similar to what we had done earlier. Next, we create a one-dimensional convolutional layer with 128 features, or kernels. The kernel size is 5 and the activation function used is sigmoid. Next, we add a global max pooling layer to reduce feature size. Finally we add a dense layer with sigmoid activation. The compilation process is the same as it was in the previous section.\n",
    "\n",
    "Let's now see the summary of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that in the above case we don't need to flatten our embedding layer. You can also notice that feature size is now reduced using the pooling layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train our model and evaluate it on the training set. The process to train and test our model remains the same. To do so, we can use the fit and evaluate methods, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script prints the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compare the training and test accuracy, you will see that the training accuracy for CNN will be around 92%, which is greater than the training accuracy of the simple neural network. The test accuracy is around 82% for the CNN, which is also greater than the test accuracy for the simple neural network, which was around 74%.\n",
    "\n",
    "However our CNN model is still overfitting as there is a vast difference between the training and test accuracy. Let's plot the loss and accuracy difference between the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc = 'upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see the loss and accuracy differences between train and test sets.\n",
    "\n",
    "Let's now train our third deep learning model, which is a recurrent neural network, and see if we can get rid of the overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification with Recurrent Neural Network (LSTM)\n",
    "Recurrent neural network is a type of neural networks that is proven to work well with sequence data. Since text is actually a sequence of words, a recurrent neural network is an automatic choice to solve text-related problems. In this section, we will use an LSTM (Long Short Term Memory network) which is a variant of RNN, to solve sentiment classification problem.\n",
    "\n",
    "Once again, execute the code until the word embedding section and after that run the following piece of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "#import lstm, time #helper libraries\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(128))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the script above, we start by initializing a sequential model followed by the creation of the embedding layer. Next, we create an LSTM layer with 128 neurons (You can play around with number of neurons). The rest of the code is same as it was for the CNN.\n",
    "\n",
    "Let's plot the summary of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to train the model on the training set and evaluate its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script above trains the model on the test set. The batch size is 128, whereas the number of epochs is 6. At the end of the training, you will see that the training accuracy is around 85.40%.\n",
    "\n",
    "Once the model is trained, we can see the model results on test set with the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output, you will see that our test accuracy is around 85.04%. The test accuracy is better than both the CNN and densely connected neural network. Also, we can see that there is a very small difference between the training accuracy and test accuracy which means that our model is not overfitting.\n",
    "\n",
    "Let's plot the loss and accuracy differences between training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows that the difference between the accuracy values for training and test sets is much smaller compared to the simple neural network and CNN. Similarly, the different between the loss values is also negligible, which shows that our model is not overfitting. We can conclude, that for our problem, RNN is the best algorithm.\n",
    "\n",
    "In this article, we randomly chose the number of layers, neurons, hyper parameters, etc. I would suggest that you try to change the number of layers, number of neurons and activation functions for all three neural networks discussed in this article and see which neural network works best for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions on Single Instance\n",
    "This is the final section of the article and here we will see how to make predictions on a single instance or single sentiment. Let's retrieve any review from our corpus and then try to predict its sentiment.\n",
    "\n",
    "Let's first randomly select any review from our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = X[2]\n",
    "print(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see that this is negative review. To predict the sentiment of this review, we have to convert this review into numeric form. We can do so using the tokenizer that we created in word embedding section. The text_to_sequences method will convert the sentence into its numeric counter part.\n",
    "\n",
    "Next, we need to pad our input sequence as we did for our corpus. Finally, we can use the predict method of our model and pass it our processed input sequence. Look at the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = tokenizer.texts_to_sequences(instance)\n",
    "\n",
    "flat_list = []\n",
    "for sublist in instance:\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)\n",
    "\n",
    "flat_list = [flat_list]\n",
    "\n",
    "instance = pad_sequences(flat_list, padding='post', maxlen=maxlen)\n",
    "\n",
    "model.predict(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we mapped the positive outputs to 1 and the negative outputs to 0. However, the sigmoid function predicts floating value between 0 and 1. If the value is less than 0.5, the sentiment is considered negative where as if the value is greater than 0.5, the sentiment is considered as positive. The sentiment value for our single instance is 0.33 which means that our sentiment is predicted as negative, which actually is the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Text classification is one of the most common natural language processing tasks. In this article we saw how to perform sentiment analysis, which is a type of text classification using Keras deep learning library. We used three different types of neural networks to classify public sentiment about different movies. The results show that LSTM, which is a variant of RNN outperforms both the CNN and simple neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
